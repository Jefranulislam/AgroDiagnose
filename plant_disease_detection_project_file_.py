# -*- coding: utf-8 -*-
"""PLANT_DISEASE_DETECTION PROJECT File .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hLtH_Fq3MuSy6sUwUEBUuahLA4McZDQR
"""

# Seeding Seeds For Reproducibility
import random
random.seed(0)

import numpy as np
np.random.seed(0)

import tensorflow as tf
tf.random.set_seed(0)

"""Importing Dependencies"""

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import pandas as pd
from PIL import Image
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import layers, models

"""Setup Kaggle API access"""

!mkdir -p ~/.kaggle
!cp "/content/AgroDiagnose/kaggle.json" ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

#Download the Plant Disease dataset
!kaggle datasets download -d abdallahalidev/plantvillage-dataset -p /content/AgroDiagnose

#Unzip the dataset
!unzip /content/AgroDiagnose/plantvillage-dataset.zip -d /content/AgroDiagnose/plantvillage-dataset

"""## Data PreProcessing"""

base_path = "/content/AgroDiagnose/plantvillage-dataset/color"
image_path = "/content/AgroDiagnose/plantvillage-dataset/color/Apple___Apple_scab/00075aa8-d81a-4184-8541-b692b78d398a___FREC_Scab 3335.JPG"

img = mpimg.imread(image_path)
print(img.shape)
plt.imshow(img)
plt.show()
plt.axis('off')

img_size = 224
batch_size =32

"""Image Generation"""

data_gen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2
)

"""Train Generator"""

train_generator = data_gen.flow_from_directory(
    base_path,
    target_size=(img_size, img_size),
    batch_size=batch_size,
    subset='training',
    class_mode = 'categorical'
)

"""Validation Generator"""

validation_generator = data_gen.flow_from_directory(
    base_path,
    target_size=(img_size, img_size),
    batch_size=batch_size,
    subset='validation',
    class_mode = 'categorical'
)

"""## Convolutional Neural Network"""

#Model
model = models.Sequential()

model.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(img_size, img_size, 3)))
model.add(layers.MaxPooling2D((2,2)))

model.add(layers.Conv2D(64, (3,3), activation='relu'))
model.add(layers.MaxPooling2D((2,2)))

model.add(layers.Flatten())
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(train_generator.num_classes, activation='softmax'))

from google.colab import drive
drive.mount('/content/drive')

model.summary()

model.compile( optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

"""# Model Training"""

history = model.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // batch_size,
    epochs=5,

    validation_data=validation_generator,
    validation_steps=validation_generator.samples // batch_size,

)

"""Model Evaluation"""

print("Evaluationg Model")

val_loss, val_accuracy = model.evaluate(validation_generator , steps = validation_generator.samples //batch_size)

print(f"Validation Accuracy: {val_accuracy* 100:.2f}%")

"""Plot"""

plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""# Building a Predictive System"""

#function to load and preprocess the image

def load_and_preprocess_image(image_path, target_size=(224,224)):

    img = Image.open(image_path)
    img = img.resize(target_size)
    img_array = np.array(img)
    img_array = np.expand_dims(img_array, axis=0)
    img_array = img_array.astype('float32') / 255.0
    return img_array

#function to predict the class of a image
def predict_iamge_class(model, image_path , class_indices):
    preprocessed_image = load_and_preprocess_image(image_path)
    prediction = model.predict(preprocessed_image)
    predicted_class_index = np.argmax(prediction, axis = 1)[0]
    predicted_class_name = class_indices[predicted_class_index]
    return predicted_class_name

#mapping
class_indices = {v: k for k, v in train_generator.class_indices.items()}

class_indices

#save Model
import json
json.dump(class_indices, open('class_indices.json', 'w'))

#image_path = "/content/038464d1-47a9-4169-afb1-72c87e568a95___RS_GLSp 4480.jpg"
image_path = "/content/AgroDiagnose/plantvillage-dataset/color/Tomato___Septoria_leaf_spot/0025c401-7785-49c5-8bef-780a8a0d3652___Matt.S_CG 2694.JPG"

predicted_class_name = predict_iamge_class(model, image_path, class_indices)

print("The predicted class is:", {predicted_class_name})

model.save('/content/AgroDiagnose/plant_disease_prediction.h5')

model.save('/content/AgroDiagnose/plant_disease_prediction.keras')

# Load the saved model and class indices
import tensorflow as tf
import json

model = tf.keras.models.load_model('/content/AgroDiagnose/plant_disease_prediction.keras')
with open('class_indices.json', 'r') as f:
    class_indices = json.load(f)
# Convert keys to integer if needed (from string during json dump)
class_indices = train_generator.class_indices

"""## Interface"""

!pip install streamlit==1.28.1

!pip install pyngrok --quiet



# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import numpy as np
# import tensorflow as tf
# from PIL import Image  # Import Image from PIL
# import json
# 
# # Load the saved model and class indices
# model = tf.keras.models.load_model('/content/drive/MyDrive/plant_disease_prediction.keras')
# with open('class_indices.json', 'r') as f:
#     class_indices = json.load(f)
# 
# # Preprocess the image
# def preprocess_image(image):
#     img = image.resize((224, 224))
#     img_array = np.array(img)
#     img_array = np.expand_dims(img_array, axis=0)
#     img_array = img_array.astype('float32') / 255.0
#     return img_array
# 
# # Make predictions
# def predict(image):
#     preprocessed_image = preprocess_image(image)
#     prediction = model.predict(preprocessed_image)
#     predicted_class_index = np.argmax(prediction, axis=1)[0]
#     predicted_class_name = class_indices.get(str(predicted_class_index), 'Unknown')
#     return predicted_class_index, predicted_class_name
# 
# # Streamlit interface
# st.title("Plant Disease Classifier")
# st.write("Upload an image of a plant leaf to classify its disease.")
# 
# uploaded_file = st.file_uploader("Choose an image...", type="jpg")
# 
# if uploaded_file is not None:
#     image = Image.open(uploaded_file)
#     st.image(image, caption='Uploaded Image.', use_column_width=True)
#     st.write("")
#     st.write("Classifying...")
#     predicted_class_index, predicted_class_name = predict(image)  # Call the predict function
#     st.write(f"Prediction Index: {predicted_class_index}")
#     st.write(f"Prediction Name: {predicted_class_name}")

ngrok.kill()

from pyngrok import ngrok

# List active tunnels
!ngrok authtoken 2wmJDJexcPAyF6hdFl6qRjgNvX9_7QMssNYuTE7EtTeUZFxJB
active_tunnels = ngrok.get_tunnels()
print("Active tunnels:", active_tunnels)

# Check if there are already 3 active tunnels
if len(active_tunnels) >= 3:
    # Disconnect the oldest tunnel (you might need to adjust the index if you want to close a different one)
    ngrok.disconnect(active_tunnels[0].public_url)
    print("Disconnected tunnel:", active_tunnels[0].public_url)

# Connect a new tunnel for your Streamlit app
public_url = ngrok.connect(8501)  # 8501 is the default port for Streamlit
print(f"Public URL: {public_url}")

!streamlit run app.py